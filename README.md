# CAISA Model Switch â€“ Streamlit + Ollama Chat App

This project is a modular **chat application** that connects to a locally running **Ollama** server and allows you to dynamically **switch between installed LLM models**, control temperature, and modify the system prompt in real time.

It is structured for clarity and future expansion on CAISA workflows, patient personas, logging, and evaluation.

---

## Features

- ğŸ” Dynamic model switching from installed Ollama models  
- ğŸ› Temperature control from the UI  
- ğŸ§  Editable system prompt (role-based behavior)  
- ğŸ’¬ Streaming responses with custom chat bubbles  
- ğŸ’¾ Persistent chat state using `st.session_state`  
- ğŸ§± Clean modular project structure  
- âš¡ GPU acceleration handled by Ollama automatically  

---

## ğŸ“ Project Structure

```text
caisa_model_switch/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ utils.py         # Ollama utilities & model fetching
â”‚   â”œâ”€â”€ state.py         # Session state management
â”‚   â”œâ”€â”€ ui.py            # UI, CSS, sidebar, chat rendering
â”‚   â””â”€â”€ chat.py          # Streaming chat logic
â”œâ”€â”€ main.py              # Streamlit entry point
â”œâ”€â”€ .env                 # Set OLLAMA_HOST server
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

